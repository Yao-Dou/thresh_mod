{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph-Level Data Processing for Thresh Annotation Interface\n",
    "\n",
    "This notebook processes legal case checklist data at the paragraph level instead of the checklist item level.\n",
    "Each instance represents a paragraph, containing all evidence-value pairs where the first evidence appears in that paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /coc/pskynet6/douy/legal-envs/bin/python3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "os.chdir('/srv/nlprx-lab/share6/douy/summarization-rl/annotation_interface/thresh_mod/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions from Original Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for JSON extraction and processing\n",
    "def remove_json_comments(text):\n",
    "    \"\"\"Remove single-line comments (//) from JSON-like text.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        string_pattern = r'\"(?:[^\"\\\\]|\\\\.)*\"'\n",
    "        strings = [(m.start(), m.end()) for m in re.finditer(string_pattern, line)]\n",
    "        \n",
    "        comment_pos = -1\n",
    "        for i in range(len(line) - 1):\n",
    "            if line[i:i+2] == '//':\n",
    "                in_string = any(start <= i < end for start, end in strings)\n",
    "                if not in_string:\n",
    "                    comment_pos = i\n",
    "                    break\n",
    "        \n",
    "        if comment_pos != -1:\n",
    "            cleaned_lines.append(line[:comment_pos].rstrip())\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"Extract JSON from text that may contain comments.\"\"\"\n",
    "    clean_text = remove_json_comments(text)\n",
    "    \n",
    "    start = clean_text.find('{')\n",
    "    end = clean_text.rfind('}')\n",
    "    \n",
    "    if start == -1 or end == -1 or start > end:\n",
    "        return None\n",
    "    \n",
    "    json_str = clean_text[start:end + 1]\n",
    "    \n",
    "    try:\n",
    "        json_obj = json.loads(json_str)\n",
    "        return json_obj\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON format - {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_checklist_item_evidence(response):\n",
    "    \"\"\"Extract checklist item from the evidence format.\"\"\"\n",
    "    json_obj = extract_json(response)\n",
    "    if json_obj:\n",
    "        return json_obj\n",
    "    else:\n",
    "        print(f\"Error: Invalid JSON format in {response}\")\n",
    "        return {}\n",
    "\n",
    "def extract_summary(text: str) -> str:\n",
    "    \"\"\"Extract summary from text that might be wrapped in markdown or other formatting.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # If text contains markdown code blocks, extract content\n",
    "    if '```' in text:\n",
    "        # Find content between first and last ```\n",
    "        parts = text.split('```')\n",
    "        if len(parts) >= 3:\n",
    "            # Content is in parts[1] if properly formatted\n",
    "            return parts[1].strip()\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Detection and Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split text into paragraphs and track their character indices.\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with keys:\n",
    "        - 'text': paragraph text\n",
    "        - 'start': start character index in original text\n",
    "        - 'end': end character index in original text (exclusive)\n",
    "        - 'paragraph_num': 0-based paragraph number\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    \n",
    "    # Try splitting by double newlines first\n",
    "    if '\\n\\n' in text:\n",
    "        splits = text.split('\\n\\n')\n",
    "        current_pos = 0\n",
    "        \n",
    "        for i, para_text in enumerate(splits):\n",
    "            if para_text.strip():  # Skip empty paragraphs\n",
    "                # Find the actual position in the original text\n",
    "                start_pos = text.find(para_text, current_pos)\n",
    "                end_pos = start_pos + len(para_text)\n",
    "                \n",
    "                paragraphs.append({\n",
    "                    'text': para_text,\n",
    "                    'start': start_pos,\n",
    "                    'end': end_pos,\n",
    "                    'paragraph_num': len(paragraphs)\n",
    "                })\n",
    "                \n",
    "                current_pos = end_pos\n",
    "    \n",
    "    # If no double newlines, try single newlines\n",
    "    elif '\\n' in text:\n",
    "        splits = text.split('\\n')\n",
    "        current_pos = 0\n",
    "        \n",
    "        for i, para_text in enumerate(splits):\n",
    "            if para_text.strip():  # Skip empty lines\n",
    "                start_pos = text.find(para_text, current_pos)\n",
    "                end_pos = start_pos + len(para_text)\n",
    "                \n",
    "                paragraphs.append({\n",
    "                    'text': para_text,\n",
    "                    'start': start_pos,\n",
    "                    'end': end_pos,\n",
    "                    'paragraph_num': len(paragraphs)\n",
    "                })\n",
    "                \n",
    "                current_pos = end_pos\n",
    "    \n",
    "    # If no newlines at all, treat entire text as one paragraph\n",
    "    else:\n",
    "        if text.strip():\n",
    "            paragraphs.append({\n",
    "                'text': text,\n",
    "                'start': 0,\n",
    "                'end': len(text),\n",
    "                'paragraph_num': 0\n",
    "            })\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def find_paragraph_for_indices(indices: List[Tuple[int, int]], paragraphs: List[Dict[str, Any]]) -> int:\n",
    "    \"\"\"\n",
    "    Find which paragraph contains the earliest evidence based on character indices.\n",
    "    \n",
    "    Args:\n",
    "        indices: List of (start, end) character positions\n",
    "        paragraphs: List of paragraph dictionaries with start/end positions\n",
    "    \n",
    "    Returns:\n",
    "        Paragraph number (0-based) that contains the earliest evidence\n",
    "    \"\"\"\n",
    "    if not indices:\n",
    "        return -1\n",
    "    \n",
    "    # Find the earliest evidence position\n",
    "    earliest_start = min(start for start, _ in indices)\n",
    "    \n",
    "    # Find which paragraph contains this position\n",
    "    for para in paragraphs:\n",
    "        # Check if the earliest evidence starts in this paragraph\n",
    "        # or spans into this paragraph\n",
    "        if para['start'] <= earliest_start < para['end']:\n",
    "            return para['paragraph_num']\n",
    "        \n",
    "        # Also check if evidence spans across paragraph boundary\n",
    "        for start, end in indices:\n",
    "            # If evidence overlaps with this paragraph at all\n",
    "            if (start < para['end'] and end > para['start']):\n",
    "                # But only if this is the earliest such paragraph\n",
    "                if start == earliest_start:\n",
    "                    return para['paragraph_num']\n",
    "    \n",
    "    # If no paragraph found (shouldn't happen), return first paragraph\n",
    "    return 0\n",
    "\n",
    "def detect_multi_paragraph_evidence(indices: List[Tuple[int, int]], paragraphs: List[Dict[str, Any]]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Detect which paragraphs an evidence spans across.\n",
    "    \n",
    "    Returns:\n",
    "        List of paragraph numbers that the evidence touches\n",
    "    \"\"\"\n",
    "    touched_paragraphs = set()\n",
    "    \n",
    "    for start, end in indices:\n",
    "        for para in paragraphs:\n",
    "            # Check if evidence overlaps with this paragraph\n",
    "            if start < para['end'] and end > para['start']:\n",
    "                touched_paragraphs.add(para['paragraph_num'])\n",
    "    \n",
    "    return sorted(list(touched_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence Finding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evidence_indices(evidence_text: str, summary: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Find all occurrences of evidence text in summary and return character indices.\n",
    "    Returns list of (start, end) tuples where end is exclusive.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    \n",
    "    # Handle evidence with ellipsis by splitting and finding each part\n",
    "    if '...' in evidence_text:\n",
    "        parts = [part.strip() for part in evidence_text.split('...') if part.strip()]\n",
    "        for part in parts:\n",
    "            # Find all occurrences of this part\n",
    "            start = 0\n",
    "            while True:\n",
    "                pos = summary.find(part, start)\n",
    "                if pos == -1:\n",
    "                    break\n",
    "                indices.append((pos, pos + len(part)))\n",
    "                start = pos + 1\n",
    "    else:\n",
    "        # Find all occurrences of the complete evidence\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = summary.find(evidence_text, start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "            indices.append((pos, pos + len(evidence_text)))\n",
    "            start = pos + 1\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def merge_overlapping_indices(indices: List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Merge overlapping or adjacent character indices.\"\"\"\n",
    "    if not indices:\n",
    "        return []\n",
    "    \n",
    "    # Sort by start position\n",
    "    sorted_indices = sorted(indices)\n",
    "    merged = [sorted_indices[0]]\n",
    "    \n",
    "    for start, end in sorted_indices[1:]:\n",
    "        last_start, last_end = merged[-1]\n",
    "        \n",
    "        # If overlapping or adjacent, merge\n",
    "        if start <= last_end:\n",
    "            merged[-1] = (last_start, max(last_end, end))\n",
    "        else:\n",
    "            merged.append((start, end))\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Function for Paragraph-Level Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checklist_category_mapping():\n",
    "    \"\"\"\n",
    "    Create a mapping from checklist item names to category names used in the YML file.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # Basic Case Information\n",
    "        \"Filing Date\": \"filing_date\",\n",
    "        \"Filing_Date\": \"filing_date\",\n",
    "        \"Who are the Parties\": \"parties\",\n",
    "        \"Parties\": \"parties\",\n",
    "        \"Class Action or Individual Plaintiffs\": \"class_action\",\n",
    "        \"Class_Action\": \"class_action\",\n",
    "        \"Type of Counsel\": \"type_of_counsel\",\n",
    "        \"Type_of_Counsel\": \"type_of_counsel\",\n",
    "        \n",
    "        # Legal Basis\n",
    "        \"Cause of Action\": \"cause_of_action\",\n",
    "        \"Cause_of_Action\": \"cause_of_action\",\n",
    "        \"Statutory or Constitutional Basis for the Case\": \"statutory_basis\",\n",
    "        \"Statutory_Constitutional_Basis\": \"statutory_basis\",\n",
    "        \"Remedy Sought\": \"remedy_sought\",\n",
    "        \"Remedy_Sought\": \"remedy_sought\",\n",
    "        \n",
    "        # Judge Information\n",
    "        \"First and Last name of Judge\": \"judge_name\",\n",
    "        \"Judge_Name\": \"judge_name\",\n",
    "        \"First and Last Name of Judge\": \"judge_name\",\n",
    "        \n",
    "        # Case Relations\n",
    "        \"Consolidated Cases Noted\": \"consolidated_cases\",\n",
    "        \"Consolidated_Cases\": \"consolidated_cases\",\n",
    "        \"Related Cases Listed by Their Case Code Number\": \"related_cases\",\n",
    "        \"Related_Cases\": \"related_cases\",\n",
    "        \n",
    "        # Filings and Rulings\n",
    "        \"Note Important Filings\": \"important_filings\",\n",
    "        \"Important_Filings\": \"important_filings\",\n",
    "        \"Court Rulings\": \"court_rulings\",\n",
    "        \"Court_Rulings\": \"court_rulings\",\n",
    "        \"All Reported Opinions Cited with Shortened Bluebook Citation\": \"reported_opinions\",\n",
    "        \"Reported_Opinions\": \"reported_opinions\",\n",
    "        \"All Reported Opinions\": \"reported_opinions\",\n",
    "        \n",
    "        # Trials and Appeals\n",
    "        \"Trials\": \"trials\",\n",
    "        \"Appeal\": \"appeals\",\n",
    "        \"Appeals\": \"appeals\",\n",
    "        \n",
    "        # Decrees\n",
    "        \"Significant Terms of Decrees\": \"decree_terms\",\n",
    "        \"Decree_Terms\": \"decree_terms\",\n",
    "        \"Dates of All Decrees\": \"decree_dates\",\n",
    "        \"Decree_Dates\": \"decree_dates\",\n",
    "        \"Dates_of_All_Decrees\": \"decree_dates\",\n",
    "        \"How Long Decrees will Last\": \"decree_duration\",\n",
    "        \"Decree_Duration\": \"decree_duration\",\n",
    "        \n",
    "        # Settlement\n",
    "        \"Significant Terms of Settlement\": \"settlement_terms\",\n",
    "        \"Settlement_Terms\": \"settlement_terms\",\n",
    "        \"Date of Settlement\": \"settlement_date\",\n",
    "        \"Settlement_Date\": \"settlement_date\",\n",
    "        \"How Long Settlement will Last\": \"settlement_duration\",\n",
    "        \"Settlement_Duration\": \"settlement_duration\",\n",
    "        \"Whether the Settlement is Court-enforced or Not\": \"court_enforced\",\n",
    "        \"Court_Enforced\": \"court_enforced\",\n",
    "        \"Whether Settlement is Court-enforced\": \"court_enforced\",\n",
    "        \"Disputes Over Settlement Enforcement\": \"settlement_disputes\",\n",
    "        \"Settlement_Disputes\": \"settlement_disputes\",\n",
    "        \n",
    "        # Monitor Information\n",
    "        \"Name of the Monitor\": \"monitor_name\",\n",
    "        \"Monitor_Name\": \"monitor_name\",\n",
    "        \"Monitor Reports\": \"monitor_reports\",\n",
    "        \"Monitor_Reports\": \"monitor_reports\",\n",
    "        \"Monitor's Reports\": \"monitor_reports\",\n",
    "        \n",
    "        # Case Facts\n",
    "        \"Factual Basis of Case\": \"factual_basis\",\n",
    "        \"Factual_Basis\": \"factual_basis\",\n",
    "        \n",
    "        # Add any additional variations you might encounter\n",
    "        \"Final_Judgment_Date\": \"decree_dates\",  # Map to decree dates if no specific category\n",
    "    }\n",
    "\n",
    "def process_checklist_to_paragraph_thresh(\n",
    "    case_id: str,\n",
    "    summary: str,\n",
    "    checklist_data: Dict[str, Any],\n",
    "    model_name: str = \"extracted_checklist\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single case with all its checklist extractions into paragraph-based thresh format.\n",
    "    \n",
    "    Args:\n",
    "        case_id: Unique identifier for the case\n",
    "        summary: The legal case summary text\n",
    "        checklist_data: Dictionary mapping checklist item names to extraction results\n",
    "        model_name: Name of the model used for extraction\n",
    "    \n",
    "    Returns:\n",
    "        List of thresh format dictionaries, one per paragraph\n",
    "    \"\"\"\n",
    "    # Get the category mapping\n",
    "    category_mapping = create_checklist_category_mapping()\n",
    "    \n",
    "    # Split summary into paragraphs\n",
    "    paragraphs = split_into_paragraphs(summary)\n",
    "    \n",
    "    if not paragraphs:\n",
    "        print(f\"Warning: No paragraphs found in summary for case {case_id}\")\n",
    "        return []\n",
    "    \n",
    "    # Initialize storage for edits by paragraph\n",
    "    paragraph_edits = defaultdict(list)\n",
    "    \n",
    "    # Process each checklist item\n",
    "    for checklist_item_name, extraction in checklist_data.items():\n",
    "        # Handle different formats of extraction\n",
    "        if isinstance(extraction, str):\n",
    "            model_output = extraction\n",
    "        elif isinstance(extraction, dict) and \"answer\" in extraction:\n",
    "            model_output = extraction[\"answer\"]\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected format for {case_id}/{checklist_item_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the model output\n",
    "        parsed_output = extract_checklist_item_evidence(model_output)\n",
    "        extracted_items = parsed_output.get(\"extracted\", [])\n",
    "        \n",
    "        # Process each extracted item\n",
    "        for item in extracted_items:\n",
    "            value = item.get(\"value\", \"\")\n",
    "            evidence_list = item.get(\"evidence\", [])\n",
    "            \n",
    "            # Convert value to string if it's not\n",
    "            if not isinstance(value, str):\n",
    "                value = repr(value)\n",
    "            \n",
    "            # Find indices for all evidence snippets\n",
    "            all_indices = []\n",
    "            for evidence in evidence_list:\n",
    "                indices = find_evidence_indices(evidence, summary)\n",
    "                all_indices.extend(indices)\n",
    "            \n",
    "            # Skip if no evidence found\n",
    "            if not all_indices:\n",
    "                continue\n",
    "            \n",
    "            # Merge overlapping indices\n",
    "            merged_indices = merge_overlapping_indices(all_indices)\n",
    "            \n",
    "            # Find which paragraph this evidence-value pair belongs to\n",
    "            # Based on the FIRST (earliest) evidence position\n",
    "            paragraph_num = find_paragraph_for_indices(merged_indices, paragraphs)\n",
    "            \n",
    "            if paragraph_num == -1:\n",
    "                print(f\"Warning: Could not find paragraph for evidence in {case_id}/{checklist_item_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Detect if evidence spans multiple paragraphs (for metadata)\n",
    "            spanned_paragraphs = detect_multi_paragraph_evidence(merged_indices, paragraphs)\n",
    "            \n",
    "            # Map checklist item name to category name\n",
    "            category = category_mapping.get(checklist_item_name, \"checklist_extraction\")\n",
    "            if category == \"checklist_extraction\":\n",
    "                # If no specific mapping found, try to create one from the name\n",
    "                # Convert to snake_case: \"Filing Date\" -> \"filing_date\"\n",
    "                category_candidate = checklist_item_name.replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "                # Remove special characters\n",
    "                category_candidate = ''.join(c if c.isalnum() or c == '_' else '' for c in category_candidate)\n",
    "                # Use the candidate if it seems reasonable\n",
    "                if category_candidate:\n",
    "                    category = category_candidate\n",
    "                    print(f\"Note: Using generated category '{category}' for checklist item '{checklist_item_name}'\")\n",
    "            \n",
    "            # Create edit for this evidence-value pair\n",
    "            edit = {\n",
    "                \"category\": category,\n",
    "                \"output_idx\": [[start, end] for start, end in merged_indices],\n",
    "                \"annotation\": {\n",
    "                    \"explanation\": value,\n",
    "                    \"checklist_item\": checklist_item_name,\n",
    "                    \"spans_paragraphs\": spanned_paragraphs if len(spanned_paragraphs) > 1 else None\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add to the appropriate paragraph\n",
    "            paragraph_edits[paragraph_num].append(edit)\n",
    "    \n",
    "    # Create thresh entries for each paragraph\n",
    "    thresh_entries = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_num = para['paragraph_num']\n",
    "        edits = paragraph_edits.get(para_num, [])\n",
    "        \n",
    "        # Assign IDs to edits\n",
    "        for i, edit in enumerate(edits):\n",
    "            edit['id'] = i\n",
    "        \n",
    "        # Create thresh entry for this paragraph\n",
    "        thresh_entry = {\n",
    "            \"id\": f\"{case_id}_p{para_num}\",\n",
    "            \"metadata\": {\n",
    "                \"case_id\": case_id,\n",
    "                \"paragraph_num\": para_num,\n",
    "                \"total_paragraphs\": len(paragraphs),\n",
    "                \"model\": model_name,\n",
    "                \"paragraph_text_preview\": para['text'][:100] + \"...\" if len(para['text']) > 100 else para['text']\n",
    "            },\n",
    "            \"source\": \"\",  # No source text in our case\n",
    "            \"target\": summary,  # Full summary for context\n",
    "            \"target_paragraph\": para['text'],  # The specific paragraph\n",
    "            \"target_paragraph_indices\": [para['start'], para['end']],  # Character indices of paragraph\n",
    "            \"edits\": edits\n",
    "        }\n",
    "        \n",
    "        thresh_entries.append(thresh_entry)\n",
    "    \n",
    "    return thresh_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paragraph_batch_separate_files(\n",
    "    checklist_data_path: str,\n",
    "    summary_data_path: str,\n",
    "    output_folder: str,\n",
    "    model_name: str = \"extracted_checklist\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a batch of cases with checklist extractions at paragraph level.\n",
    "    Each case file contains multiple instances, one for each paragraph.\n",
    "    \n",
    "    Args:\n",
    "        checklist_data_path: Path to JSON file with model extractions\n",
    "        summary_data_path: Path to JSON file with case summaries  \n",
    "        output_folder: Folder path to save individual case files\n",
    "        model_name: Name of the model used for extraction\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Load checklist data\n",
    "    with open(checklist_data_path, 'r') as f:\n",
    "        checklist_data = json.load(f)\n",
    "    \n",
    "    # Load summary data\n",
    "    with open(summary_data_path, 'r') as f:\n",
    "        summary_data = json.load(f)\n",
    "    \n",
    "    # Process summaries based on data format\n",
    "    summary_dict = {}\n",
    "    \n",
    "    # Check if data is a list (reference format) or dict (model generated format)\n",
    "    if isinstance(summary_data, list):\n",
    "        # Reference data format: list of cases with \"summary/long\" field\n",
    "        for case in summary_data:\n",
    "            case_id = case.get(\"case_id\")\n",
    "            summary = case.get(\"summary/long\")\n",
    "            if case_id and summary:\n",
    "                summary_dict[case_id] = summary\n",
    "    elif isinstance(summary_data, dict) and \"results\" in summary_data:\n",
    "        # Model generated format: dict with \"results\" containing case_id -> summary mapping\n",
    "        for case_id, summary in summary_data[\"results\"].items():\n",
    "            if isinstance(summary, dict):\n",
    "                # Summary might be wrapped in dict with \"answer\" field\n",
    "                extracted = extract_summary(summary.get(\"answer\", \"\"))\n",
    "            else:\n",
    "                # Direct summary string\n",
    "                extracted = extract_summary(summary)\n",
    "            \n",
    "            if extracted:\n",
    "                summary_dict[case_id] = extracted\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected summary data format\")\n",
    "        return 0\n",
    "    \n",
    "    # Process each case\n",
    "    processed_count = 0\n",
    "    total_paragraphs = 0\n",
    "    results = checklist_data.get(\"results\", {})\n",
    "    \n",
    "    for case_id, checklist in results.items():\n",
    "        # Get the summary for this case\n",
    "        summary = summary_dict.get(case_id)\n",
    "        if not summary:\n",
    "            print(f\"Warning: No summary found for case {case_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process into paragraph-based instances\n",
    "        paragraph_instances = process_checklist_to_paragraph_thresh(\n",
    "            case_id=case_id,\n",
    "            summary=summary,\n",
    "            checklist_data=checklist,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # Save all instances for this case to a single file\n",
    "        if paragraph_instances:\n",
    "            output_path = os.path.join(output_folder, f\"{case_id}.json\")\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(paragraph_instances, f, indent=2)\n",
    "            \n",
    "            processed_count += 1\n",
    "            total_paragraphs += len(paragraph_instances)\n",
    "            print(f\"Processed {case_id}: {len(paragraph_instances)} paragraphs\")\n",
    "    \n",
    "    print(f\"\\nTotal: Processed {processed_count} cases into {total_paragraphs} paragraph instances\")\n",
    "    print(f\"Files saved to {output_folder}/\")\n",
    "    \n",
    "    return processed_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 paragraph-based instances:\n",
      "\n",
      "Paragraph 0: This case involves a dispute over property rights between John Smith and Jane Doe.\n",
      "The parties enter...\n",
      "  Number of edits: 1\n",
      "  Checklist items in this paragraph:\n",
      "    - Filing_Date\n",
      "\n",
      "Paragraph 1: The court issued its first decree on March 15, 2023, ordering preliminary injunction.\n",
      "The injunction...\n",
      "  Number of edits: 2\n",
      "  Checklist items in this paragraph:\n",
      "    - Dates_of_All_Decrees\n",
      "\n",
      "Paragraph 2: The final judgment was entered on May 1, 2023.\n",
      "Both parties were ordered to pay equal shares of cour...\n",
      "  Number of edits: 2\n",
      "  Checklist items in this paragraph:\n",
      "    - Final_Judgment_Date\n",
      "\n",
      "\n",
      "Detailed view of first paragraph instance:\n",
      "{\n",
      "  \"id\": \"example_001_p0\",\n",
      "  \"metadata\": {\n",
      "    \"case_id\": \"example_001\",\n",
      "    \"paragraph_num\": 0,\n",
      "    \"total_paragraphs\": 3,\n",
      "    \"model\": \"example_model\",\n",
      "    \"paragraph_text_preview\": \"This case involves a dispute over property rights between John Smith and Jane Doe.\\nThe parties enter...\"\n",
      "  },\n",
      "  \"source\": \"\",\n",
      "  \"target\": \"This case involves a dispute over property rights between John Smith and Jane Doe.\\nThe parties entered into a purchase agreement on January 10, 2023.\\n\\nThe court issued its first decree on March 15, 2023, ordering preliminary injunction.\\nThe injunction prevented the sale of the property to third parties.\\nA second decree was issued on April 20, 2023, finalizing the property division.\\n\\nThe final judgment was entered on May 1, 2023.\\nBoth parties were ordered to pay equal shares of court costs.\\nThe case was officially closed on May 15, 2023.\",\n",
      "  \"target_paragraph\": \"This case involves a dispute over property rights between John Smith and Jane Doe.\\nThe parties entered into a purchase agreement on January 10, 2023.\",\n",
      "  \"target_paragraph_indices\": [\n",
      "    0,\n",
      "    149\n",
      "  ],\n",
      "  \"edits\": [\n",
      "    {\n",
      "      \"category\": \"filing_date\",\n",
      "      \"output_idx\": [\n",
      "        [\n",
      "          83,\n",
      "          149\n",
      "        ]\n",
      "      ],\n",
      "      \"annotation\": {\n",
      "        \"explanation\": \"January 10, 2023\",\n",
      "        \"checklist_item\": \"Filing_Date\",\n",
      "        \"spans_paragraphs\": null\n",
      "      },\n",
      "      \"id\": 0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example: Process a single case to demonstrate paragraph-level organization\n",
    "example_case_id = \"example_001\"\n",
    "example_summary = \"\"\"This case involves a dispute over property rights between John Smith and Jane Doe.\n",
    "The parties entered into a purchase agreement on January 10, 2023.\n",
    "\n",
    "The court issued its first decree on March 15, 2023, ordering preliminary injunction.\n",
    "The injunction prevented the sale of the property to third parties.\n",
    "A second decree was issued on April 20, 2023, finalizing the property division.\n",
    "\n",
    "The final judgment was entered on May 1, 2023.\n",
    "Both parties were ordered to pay equal shares of court costs.\n",
    "The case was officially closed on May 15, 2023.\"\"\"\n",
    "\n",
    "# Example checklist data with multiple items\n",
    "example_checklist = {\n",
    "    \"Dates_of_All_Decrees\": \"\"\"{\n",
    "        \"reasoning\": \"The summary mentions multiple decree dates.\",\n",
    "        \"extracted\": [\n",
    "            {\n",
    "                \"evidence\": [\"The court issued its first decree on March 15, 2023, ordering preliminary injunction.\"],\n",
    "                \"value\": \"March 15, 2023: preliminary injunction\"\n",
    "            },\n",
    "            {\n",
    "                \"evidence\": [\"A second decree was issued on April 20, 2023, finalizing the property division.\"],\n",
    "                \"value\": \"April 20, 2023: property division finalization\"\n",
    "            }\n",
    "        ]\n",
    "    }\"\"\",\n",
    "    \"Filing_Date\": \"\"\"{\n",
    "        \"reasoning\": \"Purchase agreement date mentioned.\",\n",
    "        \"extracted\": [\n",
    "            {\n",
    "                \"evidence\": [\"The parties entered into a purchase agreement on January 10, 2023.\"],\n",
    "                \"value\": \"January 10, 2023\"\n",
    "            }\n",
    "        ]\n",
    "    }\"\"\",\n",
    "    \"Final_Judgment_Date\": \"\"\"{\n",
    "        \"reasoning\": \"Final judgment and case closure dates found.\",\n",
    "        \"extracted\": [\n",
    "            {\n",
    "                \"evidence\": [\"The final judgment was entered on May 1, 2023.\"],\n",
    "                \"value\": \"May 1, 2023\"\n",
    "            },\n",
    "            {\n",
    "                \"evidence\": [\"The case was officially closed on May 15, 2023.\"],\n",
    "                \"value\": \"May 15, 2023: case closed\"\n",
    "            }\n",
    "        ]\n",
    "    }\"\"\"\n",
    "}\n",
    "\n",
    "# Process the example into paragraph-based format\n",
    "paragraph_entries = process_checklist_to_paragraph_thresh(\n",
    "    case_id=example_case_id,\n",
    "    summary=example_summary,\n",
    "    checklist_data=example_checklist,\n",
    "    model_name=\"example_model\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Generated {len(paragraph_entries)} paragraph-based instances:\\n\")\n",
    "for i, entry in enumerate(paragraph_entries):\n",
    "    print(f\"Paragraph {i}: {entry['metadata']['paragraph_text_preview']}\")\n",
    "    print(f\"  Number of edits: {len(entry['edits'])}\")\n",
    "    if entry['edits']:\n",
    "        print(f\"  Checklist items in this paragraph:\")\n",
    "        items = set(edit['annotation']['checklist_item'] for edit in entry['edits'])\n",
    "        for item in items:\n",
    "            print(f\"    - {item}\")\n",
    "    print()\n",
    "\n",
    "# Show detailed view of one paragraph instance\n",
    "if paragraph_entries:\n",
    "    print(\"\\nDetailed view of first paragraph instance:\")\n",
    "    print(json.dumps(paragraph_entries[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Actual Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 46014: 3 paragraphs\n",
      "Processed 45235: 3 paragraphs\n",
      "Processed 45223: 10 paragraphs\n",
      "Processed 45160: 6 paragraphs\n",
      "Processed 45157: 8 paragraphs\n",
      "Processed 45858: 7 paragraphs\n",
      "Processed 45544: 8 paragraphs\n",
      "Processed 46071: 10 paragraphs\n",
      "Processed 45429: 4 paragraphs\n",
      "Processed 45737: 9 paragraphs\n",
      "Processed 43840: 37 paragraphs\n",
      "Processed 44606: 7 paragraphs\n",
      "Processed 17762: 8 paragraphs\n",
      "Processed 46406: 5 paragraphs\n",
      "Processed 46310: 11 paragraphs\n",
      "Processed 17268: 11 paragraphs\n",
      "Processed 46083: 14 paragraphs\n",
      "Processed 45647: 6 paragraphs\n",
      "Processed 43966: 16 paragraphs\n",
      "Processed 17701: 12 paragraphs\n",
      "\n",
      "Total: Processed 20 cases into 195 paragraph instances\n",
      "Files saved to public/data/legal_extract_checklist_paragraph/legal_cases_reference/\n"
     ]
    }
   ],
   "source": [
    "# Process actual data files into paragraph-level format\n",
    "# Update these paths to your actual data locations\n",
    "\n",
    "model_name = \"gpt-5-2025-08-07\"\n",
    "\n",
    "file_name = \"2024_example_cases_20\"\n",
    "\n",
    "# For reference summaries\n",
    "checklist_path = f\"../../summary_checklist_evidence/legal/multi_lexsum/{model_name}/{file_name}_thinking_medium.json\"\n",
    "summary_path = f\"../../data/legal/multi_lexsum/{file_name}.json\"\n",
    "output_folder = \"public/data/legal_extract_checklist_paragraph/legal_cases_reference\"\n",
    "\n",
    "# Process the files\n",
    "if os.path.exists(checklist_path) and os.path.exists(summary_path):\n",
    "    processed_count = process_paragraph_batch_separate_files(\n",
    "        checklist_data_path=checklist_path,\n",
    "        summary_data_path=summary_path,\n",
    "        output_folder=output_folder,\n",
    "        model_name=model_name\n",
    "    )\n",
    "else:\n",
    "    print(\"Data files not found. Please update the paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases processed: 47\n",
      "Total paragraph instances: 420\n",
      "Average paragraphs per case: 8.94\n",
      "Total edits: 2852\n",
      "Average edits per paragraph: 6.79\n",
      "Empty paragraphs (no edits): 21\n",
      "Evidence spanning multiple paragraphs: 392\n"
     ]
    }
   ],
   "source": [
    "def analyze_paragraph_distribution(output_folder: str):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of paragraphs and edits in processed files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        print(f\"Output folder {output_folder} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    stats = {\n",
    "        'total_cases': 0,\n",
    "        'total_paragraphs': 0,\n",
    "        'total_edits': 0,\n",
    "        'paragraphs_per_case': [],\n",
    "        'edits_per_paragraph': [],\n",
    "        'empty_paragraphs': 0,\n",
    "        'multi_span_evidence': 0\n",
    "    }\n",
    "    \n",
    "    # Iterate through all JSON files in the output folder\n",
    "    for filename in os.listdir(output_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(output_folder, filename)\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            stats['total_cases'] += 1\n",
    "            stats['total_paragraphs'] += len(data)\n",
    "            stats['paragraphs_per_case'].append(len(data))\n",
    "            \n",
    "            for instance in data:\n",
    "                edits = instance.get('edits', [])\n",
    "                stats['total_edits'] += len(edits)\n",
    "                stats['edits_per_paragraph'].append(len(edits))\n",
    "                \n",
    "                if len(edits) == 0:\n",
    "                    stats['empty_paragraphs'] += 1\n",
    "                \n",
    "                # Check for multi-paragraph spanning evidence\n",
    "                for edit in edits:\n",
    "                    if edit.get('annotation', {}).get('spans_paragraphs'):\n",
    "                        stats['multi_span_evidence'] += 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    if stats['total_cases'] > 0:\n",
    "        avg_para_per_case = sum(stats['paragraphs_per_case']) / len(stats['paragraphs_per_case'])\n",
    "        print(f\"Total cases processed: {stats['total_cases']}\")\n",
    "        print(f\"Total paragraph instances: {stats['total_paragraphs']}\")\n",
    "        print(f\"Average paragraphs per case: {avg_para_per_case:.2f}\")\n",
    "    \n",
    "    if stats['edits_per_paragraph']:\n",
    "        avg_edits_per_para = sum(stats['edits_per_paragraph']) / len(stats['edits_per_paragraph'])\n",
    "        print(f\"Total edits: {stats['total_edits']}\")\n",
    "        print(f\"Average edits per paragraph: {avg_edits_per_para:.2f}\")\n",
    "        print(f\"Empty paragraphs (no edits): {stats['empty_paragraphs']}\")\n",
    "        print(f\"Evidence spanning multiple paragraphs: {stats['multi_span_evidence']}\")\n",
    "\n",
    "# Run analysis on processed data\n",
    "analyze_paragraph_distribution(\"public/data/legal_extract_checklist_paragraph/legal_cases_reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
